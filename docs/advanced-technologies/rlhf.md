# Reinforcement Learning from Human Feedback (RLHF)

RLHF aims to further refine the quality of the LLM's responses, ensuring they are **helpful, honest, and harmless**.

## How it works

### Training an Answer Quality Model

First, you should train an answer quality model.

Through **Supervised Learning** the model generates multiple responses to a prompt, and humans rate these responses based on their quality.

Finally, you should create a **Rating Scale**, the responses are scored from very good to very poor.

For example, a helpful response might score a 5, while a negative or unhelpful response scores lower.

### Using the Quality Model for Further Training

1. **Automatic Scoring**: The trained quality model can now automatically score new responses generated by the LLM.
2. **Reinforcement**: These scores act as reinforcement signals, guiding the LLM to generate better responses in the future.

By learning to generate responses that receive higher scores, the LLM becomes more adept at providing useful, honest, and harmless answers.